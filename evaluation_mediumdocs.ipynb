{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "# from langchain.chat_models import AzureChatOpenAI\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser,StructuredOutputParser,ResponseSchema\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summary.doc_summary import DocSummary\n",
    "from summary.doc_summary import summaryMetrics\n",
    "from summary.doc_summary import NLGMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scientific papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Hugging face dataset \n",
    "import pandas as pd\n",
    "\n",
    "df_papers = pd.read_csv(\"./data/scientific-papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.shape, df_papers.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Medium Documents (tokens between 3.5K and 20K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_papers[(df_papers[\"num_tokens\"] >= 3500) & (df_papers[\"num_tokens\"] < 20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected.shape,df_selected.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_summary_medium(text):\n",
    "    docSum = DocSummary(text=text)\n",
    "    return docSum.summary_medium()\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_results(summary_medium):\n",
    "    summary_dict = {}\n",
    "    summary_dict[\"summary_medium\"] = summary_medium[0]\n",
    "    summary_dict[\"response_time_summary_medium\"] = summary_medium[1]\n",
    "    return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose data points randomly\n",
    "import random\n",
    "\n",
    "# Choose 10 numbers randomly from the range 1 to 100\n",
    "random_numbers = random.sample(range(1, df_selected.shape[0]), 50)\n",
    "\n",
    "print(random_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in random_numbers:\n",
    "    print(n)\n",
    "    summary_medium = \"\"\n",
    "    text = df_selected.iloc[n][\"article\"]\n",
    "    num_tokens = df_selected.iloc[n][\"num_tokens\"]\n",
    "    print(num_tokens)\n",
    "    summary_short = get_summary_medium(text)\n",
    "    summary_dict = process_results(summary_short)\n",
    "    summary_dict[\"article\"] = df_selected.iloc[n][\"article\"]\n",
    "    summary_dict[\"abstract\"] = df_selected.iloc[n][\"abstract\"]\n",
    "    summary_dict[\"section_names\"] = df_selected.iloc[n][\"section_names\"]\n",
    "    summary_dict[\"num_tokens\"] = df_selected.iloc[n][\"num_tokens\"]\n",
    "    summary_list.append(summary_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(summary_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/paper_summary_medium_gpt354k.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data= pd.read_csv(\"./data/paper_summary_medium_gpt354k.csv\")\n",
    "df_data.shape,df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calucalte metrics\n",
    "def get_metrics(candidate,reference):\n",
    "    metrics = summaryMetrics(summary_text=candidate,reference_text=reference)\n",
    "    rouge_1_p,rouge_1_r,rouge_1_f,rouge_2_p,rouge_2_r,rouge_2_f,rouge_l_p,rouge_l_r,rouge_l_f = metrics.get_rouge_score()\n",
    "    bert_p,bert_r,bert_f = metrics.get_bert_score()\n",
    "    return  rouge_1_p,rouge_1_r,rouge_1_f,rouge_2_p,rouge_2_r,rouge_2_f,rouge_l_p,rouge_l_r,rouge_l_f,bert_p,bert_r,bert_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[['summary_medium_rouge_1_p', 'summary_medium_rouge_1_r','summary_medium_rouge_1_f','summary_medium_rouge_2_p', 'summary_medium_rouge_2_r','summary_medium_rouge_3_f','summary_medium_rouge_l_p', 'summary_medium_rouge_l_r','summary_medium_rouge_l_f','summary_medium_bert_p','summary_medium_bert_r','summary_medium_bert_f']] = df_data.apply(lambda row: pd.Series(get_metrics(row['summary_medium'],row['abstract'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns,df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(4,4))\n",
    "\n",
    "# Plot bar graph for Bert P Scores\n",
    "selected_cols_p = [\"summary_medium_bert_p\",\"summary_medium_bert_r\"]\n",
    "df_data[selected_cols_p].mean().plot(kind='bar', ax=axes, edgecolor='black')\n",
    "axes.set_title('Bert Scores')\n",
    "axes.set_ylabel('Scores')\n",
    "axes.set_ylim(0.7, 0.88)\n",
    "axes.grid(True)\n",
    "\n",
    "# Plot bar graph for Bert R Scores\n",
    "# selected_cols_r = [\"summary_short_bert_r\"]\n",
    "# df_data[selected_cols_r].mean().plot(kind='bar', ax=axes[1], edgecolor='black')\n",
    "# axes[1].set_title('Bert R Scores')\n",
    "# axes[1].set_ylabel('Scores')\n",
    "# axes[1].set_ylim(0.75, 0.82)\n",
    "axes.grid(True)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create subplots (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(4,4))\n",
    "\n",
    "# # Plot bar graph for resp time\n",
    "\n",
    "# Plot each column against column 'n'\n",
    "\n",
    "plt.scatter(df_data['num_tokens'], df_data[\"response_time_summary_medium\"],label=\"response_time_summary_medium\")\n",
    "\n",
    "\n",
    "plt.xlabel('num tokens')\n",
    "plt.ylabel('response time(sec)')\n",
    "plt.title('Response time')\n",
    "plt.legend()\n",
    "axes.set_ylim(4,30)\n",
    "plt.grid(True)\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "# plt.ylim(0.8, 0.9)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLG Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlg_metrics(doc,summary):\n",
    "    metrics = NLGMetrics(doc,summary)\n",
    "    metric_scores = metrics.get_nlg_metrics()\n",
    "    return metric_scores['coherence'],metric_scores['consistency'],metric_scores['fluency'],metric_scores['relevance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The length od the document exceeds the permissible token limit for medium documents. Hence abstract is used instead of entire article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[['coherence','consistency','fluency','relevance']] = df_data.apply(lambda row: pd.Series(nlg_metrics(row['abstract'],row['summary_medium'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.columns,df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_csv(\"./data/summary_medium_all_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Print all the available datasets\n",
    "from huggingface_hub import list_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_ds = load_dataset('scientific_papers','arxiv',split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_list = []\n",
    "\n",
    "for d in papers_ds :\n",
    "    text = d[\"article\"]\n",
    "    abstract = d[\"abstract\"]\n",
    "    section_names = d[\"section_names\"]\n",
    "    d[\"num_tokens\"] = llm.get_num_tokens(text)\n",
    "    papers_list.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.DataFrame(papers_list)\n",
    "df_raw.shape,df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.to_csv(\"./data/scientific-papers.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summaryappenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
